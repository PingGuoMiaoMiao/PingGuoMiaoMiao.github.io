---
title: "目标管线概览"
tag: "学习笔记"
date: "2025-11-19"
summary: "目标管线概览 唤醒：Porcupine 或自写 wakeword → 通知 UI 切到“倾听”表情。 STT：whisper.cpp（本地）或 Azure Speech（云端） → 输出文本、置信度。..."
status: "published"
readingTime: 11
---

# 目标管线概览

目标管线概览
唤醒：Porcupine 或自写 wakeword → 通知 UI 切到“倾听”表情。
STT：whisper.cpp（本地）或 Azure Speech（云端） → 输出文本、置信度。
回复生成：先使用规则映射，逐步替换为真实 LLM（OpenAI、Azure、Qwen 等）。
TTS 播报：Coqui/piper（本地）或 Azure/ElevenLabs → 生成 wav 音频。
Live2D 动作：根据情绪标记切表情，播放 TTS 时驱动嘴型，结束后回到 idle。
阶段拆分与实现建议
阶段 1：本地规则回复 + 对话气泡（已完成基础）
buildDialogResponse() 继续扩写关键词 → 回复 → 情绪。
默认分支也返回一句朵妹专属台词，保证任何识别结果都有回应。
voice:dialog 事件已经展示在对话气泡，可进一步设计 UI（例如叠加图标、支持多条历史）。
阶段 2：接入真实 LLM（可选）
准备：获取 OpenAI/Azure/Qwen API key（建议存放 .env）。
实现：在 buildDialogResponse() 中改为：
先把识别的文本发给 LLM（可带 prompt，如「你是朵妹酱，要温柔活泼回复」）。
拿到回复后摘要情绪（简单规则或情感模型）。
落地：
在主进程新建 callLLM(text)，内部用 fetch 或 axios 调用 API。
失败时回退到本地规则。
可选增强：记住会话上下文、加入 system prompt 控制口吻。
阶段 3：本地 TTS + 嘴型同步
选型：先用 piper 或 Coqui TTS 输出 wav。
例如安装 piper-tts, 下载中文模型，命令行：piper --model xxx.onnx --output reply.wav --text "嗨嗨"。
Node 主进程可用 child_process.spawn 生成音频，或直接调用 python 脚本。
播放与同步：
渲染层用 <audio> 或 AudioContext 播放 wav。
pixi-live2d-display 支持 model._sound = PIXI.sound.Sound.from(...) 自动驱动 ParamMouthOpenY，或手动监听音量包络设置！
流程：
voice:dialog 下发后，主进程生成 TTS 音频 → 发送 voice:tts 事件（含音频 URL）。
渲染层收到后播放，播放期间保持对应表情，结束后回 idle。
阶段 4：精准唤醒 + VAD
引入 Porcupine/Silero VAD，把唤醒与语音段切分提升稳定性。
监听器变成「Porcupine 监听」+「捕捉语音段 → 送 Whisper」。
这样即便存在背景音，也不会误触对话气泡。
代码落地建议
主进程 main.js
封装“对话引擎”：
  async function processDialog(text) {    const sanitized = sanitizeRecognizedText(text);    const ruleResult = matchRules(sanitized); // 现有逻辑    // 可选：如果配置开启 LLM，则调用 callLLM(sanitized)    return ruleResult;  }
若启用 TTS：
  async function synthesizeSpeech(reply) {    // 调用 piper/Coqui 或云端 API 生成 wav，返回文件路径或 dataURL  }
voiceProcess.stdout 收到文字后：
  const dialog = await processDialog(text);  window.webContents.send('voice:dialog', {...dialog});  const audioInfo = await synthesizeSpeech(dialog.replyText);  if (audioInfo) window.webContents.send('voice:tts', audioInfo);
渲染层 renderer.js
当前 voice:dialog 已更新对话气泡。
新增 ipcRenderer.on('voice:tts', ...)：
创建 <audio> 元素播放。
使用 audio.addEventListener('timeupdate', ...) 或 AnalyserNode 获取音量，设置 ParamMouthOpenY。
播放时锁定某种表情，结束再恢复。
配置管理
在 voice.config.json 增加开关与路径：
useLLM, llmEndpoint, llmApiKey, useTTS, ttsCommand, wakewordEngine 等。
主进程读取配置后，决定是否调用 LLM/TTS。
推荐下一步
从 规则扩展 + TTS 入手最实在：
把 buildDialogResponse() 的规则写得更细（例如针对天气、鼓励、吐槽等）。
选用 piper（或 Coqui）生成语音，在主进程调用并将音频发到渲染层。
渲染层播放音频，利用 ParamMouthOpenY 做简单嘴型同步。
完成后，你就拥有“朵妹酱会说话、会回应”的完整演示，再逐步引入 Porcupine、云端 LLM/TTS，替换本地模块即可。
如果你想先做 TTS 播报或 LLM，我可以直接帮你写具体代码；只需告诉我选哪种 TTS/LLM 服务以及相关的 API key 或模型路径。